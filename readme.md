# Multi Modal RAG for PDFs (Text & Images)

This repository implements a **Multimodal Retrieval-Augmented Generation (RAG)** pipeline capable of extracting and retrieving **both text and images** from PDF documents. It uses the Unstructured library for PDF parsing and LangChain to build a hybrid retrieval workflow that enhances large-language-model responses with grounded context from your documents.

---

## ğŸš€ Features

* Extracts **text content** from PDFs (headings, paragraphs, tables, etc.).
* Extracts **embedded images** (charts, figures, diagrams) and treats them as retrievable content.
* Builds separate embeddings (or unified embeddings) for **text + image** modalities.
* Supports **multimodal retrieval**: when a query relates to text or image, you get relevant context from either.
* Uses LangChainâ€™s loaders, vector stores and retrieval chains for rapid prototyping.
* Configurable and modular â€” you can swap out embedding models, vector-stores, or retrieval strategies.

---

## ğŸ§° Tech Stack

* **Python** â€“ core implementation language
* Unstructured â€“ for parsing PDFs (text + images)
* LangChain â€“ for document loaders, vector stores, retrieval chains
* Vector store (e.g., FAISS, Chroma) â€“ for embedding storage & retrieval
* Embedding models (e.g., from OpenAI, Huggingâ€¯Face) â€“ for text & image embedding
* Base64 or other encoding â€“ for image conversion and embedding storage
* Jupyter Notebook â€“ demonstration and interactive exploration

---

## ğŸ“ Repository Structure

```
/Multi_Modal_RAG
â”‚   README.md
â”‚   requirements.txt
â”‚   .env         â† secret keys, config
â”‚
â””â”€â”€ content/
    â””â”€â”€ langchain_multimodal.ipynb   â† interactive notebook
```

---

## ğŸ§­ Getting Started

### 1. Clone the repository

```bash
git clone https://github.com/furqank73/Multi_Modal_RAG.git
cd Multi_Modal_RAG
```

### 2. Setup environment

* Create a Python virtual environment:

  ```bash
  python3 -m venv venv
  source venv/bin/activate   # (on Windows: venv\Scripts\activate)
  ```
* Install dependencies:

  ```bash
  pip install -r requirements.txt
  ```

### 3. Configure environment variables

Create a `.env` file in the project root with your secrets and config, for example:

```dotenv
OPENAI_API_KEY=your_openai_key
EMBEDDING_MODEL=text-embedding-xyz
IMAGE_EMBEDDING_MODEL=vision-embedding-xyz
VECTOR_STORE_PATH=./vector_store
```

### 4. Run the interactive notebook

Open `content/langchain_multimodal.ipynb` in Jupyter Notebook or JupyterLab and run through the steps:

* Load a PDF (or multiple PDFs)
* Extract text + images using Unstructured
* Embed text and images
* make summaries of text and images and emmbed it
* Build a vector store
* Perform queries (text-only, image-only, or mixed)
* Retrieve relevant chunks and generate answers via an LLM

---

## ğŸ¯ Use Cases

* Research papers that include both text and figures â€” ask questions like *â€œWhat does Figure 2 show?â€*
* Corporate reports with charts and textual analysis â€” retrieve context for both modality types.
* Manuals, product catalogs, or scanned documents with mixed content â€” make them searchable.

---